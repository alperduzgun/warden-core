# Warden LLM Configuration
# ========================
# Copy this file to .env and fill in your values.

# ─────────────────────────────────────────────────────────────────
# LOCAL LLM (Ollama) - No API key needed
# ─────────────────────────────────────────────────────────────────
# Just run: ollama serve
# OLLAMA_HOST=http://localhost:11434

# ─────────────────────────────────────────────────────────────────
# CLOUD LLM OPTIONS (Choose one)
# ─────────────────────────────────────────────────────────────────

# Option 1: Anthropic Claude (Best quality)
# ANTHROPIC_API_KEY=sk-ant-...

# Option 2: OpenAI (Popular)
# OPENAI_API_KEY=sk-...

# Option 3: Groq (Fast & cheap - recommended for CI)
# GROQ_API_KEY=gsk_...

# Option 4: Azure OpenAI (Enterprise)
# AZURE_OPENAI_API_KEY=...
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4
# AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Option 5: DeepSeek (Budget)
# DEEPSEEK_API_KEY=...

# ─────────────────────────────────────────────────────────────────
# CI/CD Configuration
# ─────────────────────────────────────────────────────────────────
# Add these to your CI secrets (GitHub Secrets, GitLab CI Variables, etc.)
# CI_LLM_PROVIDER=groq
# GROQ_API_KEY=gsk_... (for CI)
